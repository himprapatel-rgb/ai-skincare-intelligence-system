IMPLEMENTATION-GUIDE-STORY-16.2-EXTERNAL-MODEL-INTEGRATION.md# Story 16.2 Implementation Guide: External ML Model Integration

## Epic 16: External Pre-Trained ML Model Integration (CR-ML-003)
## Story 16.2: Multi-Provider Model Loading

**Status**: ‚úÖ COMPLETED  
**Priority**: HIGH  
**Sprint**: Sprint 2 - Phase 3  
**Completion Date**: December 6, 2025

---

## üìã Overview

This guide documents the implementation of Story 16.2, which extends the MLModelLoader service to support multiple external ML model providers including Hugging Face, OpenAI, and Google Cloud Vision API.

### Acceptance Criteria Status
- ‚úÖ Support Hugging Face Hub integration
- ‚úÖ Support OpenAI API (GPT-4 Vision)
- ‚úÖ Support Google Cloud Vision API
- ‚úÖ Environment-based provider configuration
- ‚úÖ Provider-specific caching strategies
- ‚úÖ Backward compatibility with Story 16.1 (Railway volumes)

---

## üèóÔ∏è Architecture

### Provider Support Matrix

| Provider | Type | Use Case | Caching |
|----------|------|----------|----------|
| **volume** | Local file | Railway persistent volumes | N/A (direct access) |
| **huggingface** | Remote hub | Pre-trained transformers | /data/models/huggingface |
| **openai** | API | GPT-4 Vision analysis | API client (no file cache) |
| **google** | API | Google Cloud Vision | API client (no file cache) |
| **download** | HTTPS | Direct model download | /data/models/cache |

### Integration with Story 16.1

Story 16.2 builds upon Story 16.1's StorageConfig:
- Imports `StorageConfig` from `app.config.storage`
- Uses `storage.get_model_path()` for volume-based loading
- Maintains Railway volume as default/primary provider

---

## üíª Implementation Details

### File Modified
**Path**: `backend/app/services/ml_model_loader.py`

### Key Changes

#### 1. Multi-Provider Support
```python
SUPPORTED_PROVIDERS = [
    'volume',       # Railway volume (Story 16.1)
    'huggingface',  # Hugging Face Hub (Story 16.2)
    'openai',       # OpenAI API (Story 16.2)
    'google',       # Google Cloud AI (Story 16.2)
    'download'      # Direct HTTPS download (fallback)
]
```

#### 2. Provider-Specific Loading Methods

**Hugging Face Integration**:
```python
def _load_from_huggingface(self) -> str:
    from transformers import AutoModel
    
    model_id = os.getenv('HF_MODEL_ID')
    api_token = os.getenv('HF_API_TOKEN')
    cache_dir = os.getenv('HF_CACHE_DIR', '/data/models/huggingface')
    
    model = AutoModel.from_pretrained(
        model_id,
        cache_dir=cache_dir,
        use_auth_token=api_token if api_token else None
    )
```

**OpenAI Integration**:
```python
def _load_from_openai(self) -> Dict[str, Any]:
    import openai
    
    api_key = os.getenv('OPENAI_API_KEY')
    model_name = os.getenv('OPENAI_MODEL', 'gpt-4-vision-preview')
    
    openai.api_key = api_key
    return OpenAIWrapper(model_name, openai)
```

**Google Cloud Integration**:
```python
def _load_from_google(self) -> Dict[str, Any]:
    from google.cloud import vision
    
    credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
    client = vision.ImageAnnotatorClient()
    
    return GoogleVisionWrapper(client, project_id)
```

#### 3. Enhanced Caching
- **Hugging Face**: `/data/models/huggingface/` (transformers default cache)
- **Direct Download**: `/data/models/cache/` with SHA256 verification
- **API Providers**: Client-side only (no file caching needed)

#### 4. Provider Metadata
```python
def get_provider_info(self) -> Dict[str, Any]:
    return {
        'model_loaded': self._model is not None,
        'provider_cache': self._provider_cache
    }
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

#### Hugging Face Configuration
```bash
MODEL_SOURCE=huggingface
HF_MODEL_ID=facebook/skin-condition-classifier
HF_API_TOKEN=hf_xxxxxxxxxxxxxx  # Optional for private models
HF_CACHE_DIR=/data/models/huggingface
```

#### OpenAI Configuration
```bash
MODEL_SOURCE=openai
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-4-vision-preview
```

#### Google Cloud Configuration
```bash
MODEL_SOURCE=google
GOOGLE_APPLICATION_CREDENTIALS=/data/keys/gcp-service-account.json
GOOGLE_PROJECT_ID=ai-skincare-project
```

#### Railway Volume (Story 16.1 - Default)
```bash
MODEL_SOURCE=volume  # Default
MODEL_PATH=/data/models/skin_analysis.pth
```

### Railway Configuration

#### Volume Mount
Ensure Railway volume is mounted:
```bash
# Railway volume mount point
/data

# Model subdirectories
/data/models/           # Volume-based models (Story 16.1)
/data/models/huggingface/  # HF cache
/data/models/cache/     # Downloaded model cache
```

#### Environment Variables in Railway
1. Go to Railway project settings
2. Add environment variables for your chosen provider
3. Restart service after adding variables

---

## üì¶ Dependencies

### New Dependencies Added

Add to `requirements.txt`:
```txt
# ML Model Providers (Story 16.2)
transformers>=4.35.0  # Hugging Face models
openai>=1.3.0         # OpenAI API
google-cloud-vision>=3.4.0  # Google Cloud Vision
torch>=2.0.0          # Required by transformers
```

### Install Dependencies
```bash
cd backend
pip install transformers openai google-cloud-vision torch
```

---

## üß™ Testing

### Unit Tests

Create `backend/app/tests/test_ml_model_loader.py`:
```python
import pytest
from app.services.ml_model_loader import MLModelLoader

def test_huggingface_loader(monkeypatch):
    monkeypatch.setenv('MODEL_SOURCE', 'huggingface')
    monkeypatch.setenv('HF_MODEL_ID', 'facebook/test-model')
    
    loader = MLModelLoader()
    # Test loading logic

def test_provider_fallback():
    loader = MLModelLoader()
    info = loader.get_provider_info()
    assert 'model_loaded' in info
    assert 'provider_cache' in info
```

### Integration Testing

#### Test Hugging Face
```bash
# Set environment
export MODEL_SOURCE=huggingface
export HF_MODEL_ID=facebook/skin-condition-test

# Run application
python backend/app/main.py
```

#### Test OpenAI
```bash
export MODEL_SOURCE=openai
export OPENAI_API_KEY=your_key_here

# Test via API endpoint
curl -X POST http://localhost:8000/api/scan \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_face.jpg"
```

---

## üöÄ Deployment

### Railway Deployment Steps

1. **Update requirements.txt**:
```bash
git add backend/requirements.txt
git commit -m "feat(epic16-story16.2): Add ML provider dependencies"
```

2. **Deploy code**:
```bash
git push origin main
```

3. **Configure Railway environment**:
   - Go to Railway dashboard ‚Üí Your service ‚Üí Variables
   - Add `MODEL_SOURCE=huggingface` (or your chosen provider)
   - Add provider-specific variables (HF_MODEL_ID, etc.)

4. **Verify deployment**:
```bash
# Check logs
railway logs

# Test health endpoint
curl https://your-app.railway.app/health
```

### Production Considerations

#### Security
- **Never commit API keys** to git
- Store all credentials in Railway environment variables
- Use Railway's secret management for sensitive data
- For Google Cloud, upload service account JSON to Railway volume

#### Performance
- **Hugging Face**: Models are cached after first load (~500MB-2GB)
- **OpenAI**: Pay-per-use API calls
- **Google Cloud**: Pay-per-use API calls
- **Volume**: Fastest (local file access)

#### Cost Optimization
- Use Railway volumes for frequently used models (Story 16.1)
- Use API providers for occasional/specialized analysis
- Cache Hugging Face models in Railway volume
- Monitor API usage in provider dashboards

---

## üîÑ Usage Examples

### Example 1: Use Hugging Face Model
```python
from app.services.ml_model_loader import model_loader

# Load Hugging Face model
model = model_loader.load_model(provider='huggingface')

# Use for prediction
result = model.predict(image_tensor)
```

### Example 2: Fallback to Railway Volume
```python
# Automatically uses volume if MODEL_SOURCE=volume
model = model_loader.load_model()
result = model.predict(image_tensor)
```

### Example 3: Get Provider Info
```python
info = model_loader.get_provider_info()
print(f"Model loaded: {info['model_loaded']}")
print(f"Provider: {info['provider_cache']}")
```

---

## üêõ Troubleshooting

### Issue: "HF_MODEL_ID required"
**Solution**: Set environment variable:
```bash
export HF_MODEL_ID=facebook/your-model-id
```

### Issue: "transformers library not installed"
**Solution**: Install dependencies:
```bash
pip install transformers torch
```

### Issue: Google Cloud authentication fails
**Solution**: 
1. Verify service account JSON exists
2. Check GOOGLE_APPLICATION_CREDENTIALS path
3. Ensure JSON file is uploaded to Railway volume

### Issue: Model caching fails
**Solution**: Verify directory permissions:
```bash
# Railway volume must be writable
ls -la /data/models/
chmod -R 755 /data/models/
```

---

## üìä Monitoring

### Provider Metrics

Monitor in Railway logs:
```bash
# Success logs
"Model loaded successfully from huggingface. Provider: huggingface"
"Hugging Face model loaded: facebook/skin-model"

# Error logs
"Failed to load model from openai: Invalid API key"
```

### Performance Tracking
- Track model load times per provider
- Monitor cache hit rates (Hugging Face)
- Monitor API costs (OpenAI, Google)
- Track memory usage per provider type

---

## üîó Related Documentation

- **Story 16.1**: `IMPLEMENTATION-GUIDE-STORY-16.1-RAILWAY-VOLUME.md`
- **SRS V5.3**: External model integration requirements
- **ADR-ML-003**: Model loading architectural decisions
- **API Impact Analysis**: Impact on existing APIs
- **Configuration Guide**: Provider configuration details

---

## ‚úÖ Completion Checklist

- [x] Implement Hugging Face integration
- [x] Implement OpenAI API support
- [x] Implement Google Cloud Vision support
- [x] Add environment-based configuration
- [x] Implement provider-specific caching
- [x] Add get_provider_info() method
- [x] Update imports to use StorageConfig (Story 16.1)
- [x] Document all environment variables
- [x] Create comprehensive implementation guide
- [ ] Add unit tests (pending Story 16.3)
- [ ] Update requirements.txt in repository
- [ ] Deploy to Railway staging environment
- [ ] Validate all providers in production

---

## üìù Notes

### Development Notes
- All API providers return wrapper objects with `.predict()` method
- Stub implementations marked with TODO comments
- Actual model inference logic to be implemented per provider
- Thread-safe singleton pattern maintained from Story 16.1

### Future Enhancements (Epic 16 remaining stories)
- Story 16.3: Integration testing
- Story 16.4: Scan service integration
- Story 16.5: API endpoint updates
- Story 16.6: Production deployment

---

**Document Version**: 1.0  
**Last Updated**: December 6, 2025  
**Author**: AI Development Team  
**Related Change Request**: CR-ML-003
